{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi Label Debiasing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGbH5Xw1su5H",
        "colab_type": "code",
        "outputId": "b9f6fdcd-f727-469d-c944-a509852d8537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "word = F\"/content/gdrive/My Drive/NLP/Project/reddit.US.txt.tok.clean.cleanedforw2v_3.w2v\" \n",
        "vocab = F\"/content/gdrive/My Drive/NLP/Project/race_attributes_optm.json\" \n",
        "path = F\"/content/gdrive/My Drive/NLP/Project/output/race/w2_v3/\"\n",
        "outprefix = 'race'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m0S22b3vI2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_legacy_w2v(w2v_file, dim=50):\n",
        "    vectors = {}\n",
        "    with open(w2v_file, 'r') as f:\n",
        "        for line in f:\n",
        "            vect = line.strip().rsplit()\n",
        "            word = vect[0]\n",
        "            vect = np.array([float(x) for x in vect[1:]])\n",
        "            if(dim == len(vect)):\n",
        "                vectors[word] = vect\n",
        "        \n",
        "    return vectors, dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZLE_7T1vJ2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors, embedding_dim  = load_legacy_w2v(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L2rHe1Fv4JF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_analogy_templates(json_filepath, mode):\n",
        "\twith open(json_filepath, \"r\") as f:\t\n",
        "\t\tloadedData = json.load(f)\n",
        "\t\treturn loadedData[\"analogy_templates\"][mode]\n",
        "\n",
        "def load_test_terms(json_filepath):\n",
        "\twith open(json_filepath, \"r\") as f:\t\n",
        "\t\tloadedData = json.load(f)\n",
        "\t\treturn loadedData[\"testTerms\"]\n",
        "\n",
        "def load_eval_terms(json_filepath, mode):\n",
        "\twith open(json_filepath, \"r\") as f:\t\n",
        "\t\tloadedData = json.load(f)\n",
        "\t\treturn loadedData[\"eval_targets\"], loadedData[\"analogy_templates\"][mode].values()\n",
        "\n",
        "def load_def_sets(json_filepath):\n",
        "\twith open(json_filepath, \"r\") as f: \n",
        "\t\tloadedData = json.load(f)\n",
        "\t\treturn {i: v for i, v in enumerate(loadedData[\"definite_sets\"])}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la9dEN88v94U",
        "colab_type": "code",
        "outputId": "1848970b-3a0c-47c1-df5c-4a0d98543e2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Loading vocabulary from {}\".format(vocab))\n",
        "\n",
        "analogyTemplates = load_analogy_templates(vocab, 'role')\n",
        "defSets = load_def_sets(vocab)\n",
        "testTerms = load_test_terms(vocab)\n",
        "neutral_words = []\n",
        "for value in analogyTemplates.values():\n",
        "    neutral_words.extend(value)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading vocabulary from /content/gdrive/My Drive/NLP/Project/race_attributes_optm.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwBbaDiyyFvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pruneWordVecs(wordVecs):\n",
        "    newWordVecs = {}\n",
        "    for word, vec in wordVecs.items():\n",
        "        valid=True\n",
        "        if(not isValidWord(word)):\n",
        "            valid = False\n",
        "        if(valid):\n",
        "            newWordVecs[word] = vec\n",
        "    return newWordVecs\n",
        "\n",
        "def isValidWord(word):\n",
        "    return all([c.isalpha() for c in word])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKFufi5_x8wX",
        "colab_type": "code",
        "outputId": "edb485a4-7728-4382-9d0c-b61685f33c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Pruning Word Vectors... Starting with\", len(word_vectors))\n",
        "word_vectors = pruneWordVecs(word_vectors)\n",
        "print(\"\\tEnded with\", len(word_vectors))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning Word Vectors... Starting with 44895\n",
            "\tEnded with 44895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgOt4mk-yg1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torch\n",
        "\n",
        "def normalize(word_vectors):\n",
        "    for k, v in word_vectors.items():\n",
        "        word_vectors[k] = v / np.linalg.norm(v)\n",
        "\n",
        "def identify_bias_subspace(vocab, def_sets, subspace_dim, embedding_dim):\n",
        "    # calculate means of defining sets\n",
        "    means = {}\n",
        "    for k, v in def_sets.items():\n",
        "        wSet = []\n",
        "        for w in v:\n",
        "            try:\n",
        "                wSet.append(vocab[w])\n",
        "            except KeyError as e:\n",
        "                pass\n",
        "        set_vectors = np.array(wSet)\n",
        "        means[k] = np.mean(set_vectors, axis=0)\n",
        "\n",
        "    # calculate vectors to perform PCA\n",
        "    matrix = []\n",
        "    for k, v in def_sets.items():\n",
        "        wSet = []\n",
        "        for w in v:\n",
        "            try:\n",
        "                wSet.append(vocab[w])\n",
        "            except KeyError as e:\n",
        "                pass\n",
        "        set_vectors = np.array(wSet)\n",
        "        diffs = set_vectors - means[k]\n",
        "        matrix.append(diffs)\n",
        "\n",
        "    matrix = np.concatenate(matrix)\n",
        "\n",
        "    pca = PCA(n_components=subspace_dim)\n",
        "    pca.fit(matrix)\n",
        "\n",
        "    return pca.components_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB1T1HXWyXdU",
        "colab_type": "code",
        "outputId": "553a5d33-4d5a-4e8d-e7ab-a7faf72c29a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Identifying bias subspace\")\n",
        "subspace = identify_bias_subspace(word_vectors, defSets, 2, embedding_dim)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Identifying bias subspace\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdVMhknxzTyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neutralize_and_equalize(vocab, words, eq_sets, bias_subspace, embedding_dim):\n",
        "    \n",
        "    if bias_subspace.ndim == 1:\n",
        "        bias_subspace = np.expand_dims(bias_subspace, 0)\n",
        "    elif bias_subspace.ndim != 2:\n",
        "        raise ValueError(\"bias subspace should be either a matrix or vector\")\n",
        "\n",
        "    new_vocab = vocab.copy()\n",
        "    for w in words:\n",
        "        # get projection onto bias subspace\n",
        "        if w in vocab:\n",
        "            v = vocab[w]\n",
        "            v_b = project_onto_subspace(v, bias_subspace)\n",
        "\n",
        "            new_v = (v - v_b) / np.linalg.norm(v - v_b)\n",
        "            #print np.linalg.norm(new_v)\n",
        "            # update embedding\n",
        "            new_vocab[w] = new_v\n",
        "\n",
        "    normalize(new_vocab)\n",
        "\n",
        "    for eq_set in eq_sets:\n",
        "        mean = np.zeros((embedding_dim,))\n",
        "\n",
        "        #Make sure the elements in the eq sets are valid\n",
        "        cleanEqSet = []\n",
        "        for w in eq_set:\n",
        "            try:\n",
        "                _ = new_vocab[w]\n",
        "                cleanEqSet.append(w)\n",
        "            except KeyError as e:\n",
        "                pass\n",
        "\n",
        "        for w in cleanEqSet:\n",
        "            mean += new_vocab[w]\n",
        "        mean /= float(len(cleanEqSet))\n",
        "\n",
        "        mean_b = project_onto_subspace(mean, bias_subspace)\n",
        "        upsilon = mean - mean_b\n",
        "\n",
        "        for w in cleanEqSet:\n",
        "            v = new_vocab[w]\n",
        "            v_b = project_onto_subspace(v, bias_subspace)\n",
        "\n",
        "            frac = (v_b - mean_b) / np.linalg.norm(v_b - mean_b)\n",
        "            new_v = upsilon + np.sqrt(1 - np.sum(np.square(upsilon))) * frac\n",
        "\n",
        "            new_vocab[w] = new_v\n",
        "\n",
        "    return new_vocab\n",
        "\n",
        "def project_onto_subspace(vector, subspace):\n",
        "    v_b = np.zeros_like(vector)\n",
        "    for component in subspace:\n",
        "        v_b += np.dot(vector.transpose(), component) * component\n",
        "    return v_b\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liN_DqU2zEMy",
        "colab_type": "code",
        "outputId": "1fd32e86-b639-4842-a2d8-3f69964bdc9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Neutralizing and Equalizing\")\n",
        "new_hard_word_vectors = neutralize_and_equalize(word_vectors, neutral_words,\n",
        "                        defSets.values(), subspace, embedding_dim)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neutralizing and Equalizing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YATV4TqCXnFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
        "\n",
        "def convert_legacy_to_keyvec(legacy_w2v):\n",
        "    dim = len(legacy_w2v[list(legacy_w2v.keys())[0]])\n",
        "    vectors = Word2VecKeyedVectors(dim)\n",
        "\n",
        "    ws = []\n",
        "    vs = []\n",
        "\n",
        "    for word, vect in legacy_w2v.items():\n",
        "        ws.append(word)\n",
        "        vs.append(vect)\n",
        "        assert(len(vect) == dim)\n",
        "    vectors.add(ws, vs, replace=True)\n",
        "    return vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDfuQVWnBDHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cos(a,b,x,y, vectors):\n",
        "\t\taVec = np.array(vectors[a])\n",
        "\t\tbVec = np.array(vectors[b])\n",
        "\t\txVec = np.array(vectors[x])\n",
        "\t\tyVec = np.array(vectors[y])\n",
        "\t\tnumerator = (aVec-bVec).dot(xVec-yVec)\n",
        "\t\tdenominator = np.linalg.norm(aVec-bVec)*np.linalg.norm(xVec-yVec)\n",
        "\t\treturn numerator/(denominator if denominator != 0 else 1e-6)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAgP2z8mXKfU",
        "colab_type": "code",
        "outputId": "9eb70fca-c8c0-4fe2-b99c-36755e64ebc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "keyedVecs = convert_legacy_to_keyvec(word_vectors)\n",
        "new_vectors = convert_legacy_to_keyvec(new_hard_word_vectors)\n",
        "cosine = -10\n",
        "final_word = final_word1 = ''\n",
        "sequence = ['black', 'musician', 'asian']\n",
        "#sequence = ['black', 'runner', 'caucasian']\n",
        "for word in keyedVecs.vocab:\n",
        "  x = cos(sequence[0], sequence[1], sequence[2], word, keyedVecs)\n",
        "  if(x > cosine):\n",
        "    cosine = x\n",
        "    final_word = word\n",
        "\n",
        "print(final_word)\n",
        "cosine = -10\n",
        "for word in new_vectors.vocab:\n",
        "  y = cos(sequence[0], sequence[1], sequence[2], word, new_vectors)  \n",
        "  if(y > cosine):\n",
        "    cosine = y\n",
        "    final_word1 = word\n",
        "\n",
        "print(final_word1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "programmer\n",
            "musician\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}